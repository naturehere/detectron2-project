{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/facebookresearch/detectron2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml==5.1 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (5.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'detectron2''...\n",
      "fatal: protocol ''https' is not supported\n",
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install pyyaml==5.1\n",
    "import sys, os, distutils.core\n",
    "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
    "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
    "!git clone 'https://github.com/facebookresearch/detectron2'\n",
    "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
    "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
    "sys.path.insert(0, os.path.abspath('./detectron2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (0.21.0+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: torch in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (0.21.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: json5 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from pandas->datasets) (2025.1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PyTorch 설치 (CUDA 11.8 기준, 필요 시 공식 사이트에서 버전 확인)\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Detectron2 설치 (Colab 환경)\n",
    "!pip install torch torchvision\n",
    "# !pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "\n",
    "# OpenCV 및 기타 라이브러리 설치\n",
    "!pip install opencv-python numpy json5 tqdm matplotlib datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install fvcore \n",
    "pip install cloudpickle\n",
    "pip install omegaconf \n",
    "pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.structures import BoxMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON Files: 100%|██████████| 1/1 [00:25<00:00, 25.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ COCO 데이터 저장 완료: C:\\Users\\nature\\Documents\\detectron2\\dp-bench\\coco\\coco_annotations.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 원본 JSON 폴더 및 이미지 폴더 경로\n",
    "json_dir = r\"C:\\Users\\nature\\Documents\\detectron2\\dp-bench\\dataset\\json\"\n",
    "image_dir = r\"C:\\Users\\nature\\Documents\\detectron2\\dp-bench\\dataset\\images\"\n",
    "\n",
    "# COCO JSON 저장 경로\n",
    "coco_output_path = r\"C:\\Users\\nature\\Documents\\detectron2\\dp-bench\\coco\\coco_annotations.json\"\n",
    "\n",
    "# COCO JSON 구조 초기화\n",
    "coco_format = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": []\n",
    "}\n",
    "\n",
    "# 카테고리 매핑\n",
    "category_mapping = {}\n",
    "category_id = 1\n",
    "annotation_id = 1\n",
    "image_id = 1\n",
    "\n",
    "# JSON 파일 목록 가져오기\n",
    "json_files = [f for f in os.listdir(json_dir) if f.endswith(\".json\")]\n",
    "\n",
    "for json_file in tqdm(json_files, desc=\"Processing JSON Files\"):\n",
    "    json_path = os.path.join(json_dir, json_file)\n",
    "    \n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    for pdf_name, pdf_data in dataset.items():\n",
    "        for element in pdf_data[\"elements\"]:\n",
    "            page_num = element[\"page\"]\n",
    "            img_filename = f\"{pdf_name.replace('.pdf', '')}_page_{page_num}.jpg\"\n",
    "\n",
    "            # 이미지 정보 추가\n",
    "            if not any(img[\"file_name\"] == img_filename for img in coco_format[\"images\"]):\n",
    "                coco_format[\"images\"].append({\n",
    "                    \"id\": image_id,\n",
    "                    \"file_name\": img_filename,\n",
    "                    \"width\": 2480,\n",
    "                    \"height\": 3508\n",
    "                })\n",
    "\n",
    "            # 카테고리 추가\n",
    "            label = element[\"category\"]\n",
    "            if label not in category_mapping:\n",
    "                category_mapping[label] = category_id\n",
    "                coco_format[\"categories\"].append({\"id\": category_id, \"name\": label})\n",
    "                category_id += 1\n",
    "\n",
    "            # 좌표 변환 (Bounding Box 변환)\n",
    "            x_min = min(coord[\"x\"] for coord in element[\"coordinates\"]) * 2480\n",
    "            y_min = min(coord[\"y\"] for coord in element[\"coordinates\"]) * 3508\n",
    "            x_max = max(coord[\"x\"] for coord in element[\"coordinates\"]) * 2480\n",
    "            y_max = max(coord[\"y\"] for coord in element[\"coordinates\"]) * 3508\n",
    "\n",
    "            bbox = [x_min, y_min, x_max - x_min, y_max - y_min]\n",
    "\n",
    "            # ✅ `segmentation` 필드 추가 (간단한 다각형 형태로 변환)\n",
    "            segmentation = [[\n",
    "                x_min, y_min, x_max, y_min, x_max, y_max, x_min, y_max\n",
    "            ]]\n",
    "\n",
    "            # 어노테이션 추가\n",
    "            coco_format[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_mapping[label],\n",
    "                \"bbox\": bbox,\n",
    "                \"area\": bbox[2] * bbox[3],\n",
    "                \"segmentation\": segmentation,  # ✅ 추가됨\n",
    "                \"iscrowd\": 0,\n",
    "                \"text\": element[\"content\"][\"text\"]\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "        image_id += 1\n",
    "\n",
    "# COCO JSON 저장\n",
    "with open(coco_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(coco_format, f, indent=4)\n",
    "\n",
    "print(f\"✅ COCO 데이터 저장 완료: {coco_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PDFs to Images: 100%|██████████| 200/200 [01:03<00:00,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF -> 이미지 변환 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PDF 및 이미지 디렉토리 설정\n",
    "pdf_dir = r\"C:\\Users\\nature\\Documents\\detectron2\\dp-bench\\dataset\\pdfs\"\n",
    "image_output_dir = r\"C:\\Users\\nature\\Documents\\detectron2\\dp-bench\\dataset\\images\"\n",
    "\n",
    "os.makedirs(image_output_dir, exist_ok=True)\n",
    "\n",
    "# PDF 파일 목록 가져오기\n",
    "pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]\n",
    "\n",
    "# PDF를 이미지로 변환\n",
    "for pdf_file in tqdm(pdf_files, desc=\"Converting PDFs to Images\"):\n",
    "    pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "    images = convert_from_path(pdf_path, dpi=300)  # PDF를 고해상도 이미지로 변환\n",
    "\n",
    "    # 변환된 이미지를 저장\n",
    "    for i, img in enumerate(images):\n",
    "        img_filename = f\"{pdf_file.replace('.pdf', '')}_page_{i+1}.jpg\"\n",
    "        img_path = os.path.join(image_output_dir, img_filename)\n",
    "        img.save(img_path, \"JPEG\")\n",
    "\n",
    "print(\"PDF -> 이미지 변환 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔹 2. Detectron2에서 Custom COCO Dataset 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/12 06:54:18 d2.data.datasets.coco]: \u001b[0mLoaded 200 images in COCO format from C:\\Users\\nature\\Documents\\detectron2\\dp-bench\\coco\\coco_annotations.json\n",
      "Detectron2 데이터셋 등록 완료!\n"
     ]
    }
   ],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "# Detectron2 데이터셋 등록\n",
    "register_coco_instances(\"custom_doclaynet\", {}, coco_output_path, image_dir)\n",
    "\n",
    "# 데이터셋 확인\n",
    "dataset_metadata = MetadataCatalog.get(\"custom_doclaynet\")\n",
    "dataset_dicts = DatasetCatalog.get(\"custom_doclaynet\")\n",
    "\n",
    "print(\"Detectron2 데이터셋 등록 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카테고리 목록: ['table', 'paragraph', 'heading1', 'header', 'list', 'figure', 'caption', 'chart', 'footer', 'footnote', 'equation', 'index']\n"
     ]
    }
   ],
   "source": [
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "# COCO 데이터셋 등록 후, thing_classes를 직접 추가\n",
    "metadata = MetadataCatalog.get(\"custom_doclaynet\")\n",
    "\n",
    "# COCO JSON에서 카테고리 목록 가져오기\n",
    "import json\n",
    "\n",
    "coco_json_path = (\n",
    "    r\"C:\\Users\\nature\\Documents\\detectron2\\dp-bench\\coco\\coco_annotations.json\"\n",
    ")\n",
    "with open(coco_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# COCO JSON에서 카테고리 추출하여 thing_classes 설정\n",
    "categories = [cat[\"name\"] for cat in coco_data[\"categories\"]]\n",
    "metadata.thing_classes = categories  # ✅ 직접 설정\n",
    "\n",
    "print(\"카테고리 목록:\", metadata.thing_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ COCO JSON 업데이트 완료! 이제 Detectron2에서 실행해보세요.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "# 이미지 및 COCO JSON 파일 경로\n",
    "image_dir = r\"C:\\Users\\nature\\Documents\\detectron2\\dp-bench\\dataset\\images\"\n",
    "coco_json_path = r\"C:\\Users\\nature\\Documents\\detectron2\\dp-bench\\coco\\coco_annotations.json\"\n",
    "\n",
    "# COCO JSON 파일 로드\n",
    "with open(coco_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# 이미지 크기 확인 및 COCO JSON 수정\n",
    "for img in coco_data[\"images\"]:\n",
    "    img_path = os.path.join(image_dir, img[\"file_name\"])\n",
    "\n",
    "    if os.path.exists(img_path):\n",
    "        # 이미지 로드 및 크기 확인\n",
    "        image = cv2.imread(img_path)\n",
    "        height, width, _ = image.shape\n",
    "\n",
    "        # COCO JSON 업데이트\n",
    "        img[\"width\"] = width\n",
    "        img[\"height\"] = height\n",
    "\n",
    "# 수정된 COCO JSON 저장\n",
    "with open(coco_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(coco_data, f, indent=4)\n",
    "\n",
    "print(\"✅ COCO JSON 업데이트 완료! 이제 Detectron2에서 실행해보세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/12 06:55:49 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=13, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=48, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[02/12 06:55:49 d2.data.datasets.coco]: \u001b[0mLoaded 200 images in COCO format from C:\\Users\\nature\\Documents\\detectron2\\dp-bench\\coco\\coco_annotations.json\n",
      "\u001b[32m[02/12 06:55:49 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 200 images left.\n",
      "\u001b[32m[02/12 06:55:49 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[02/12 06:55:49 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[02/12 06:55:49 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[02/12 06:55:49 d2.data.common]: \u001b[0mSerializing 200 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[02/12 06:55:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.29 MiB\n",
      "\u001b[32m[02/12 06:55:49 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=2\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[02/12 06:55:49 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "\u001b[32m[02/12 06:55:50 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (13, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (13,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (48, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (48,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (12, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (12,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/12 06:55:50 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "\u001b[32m[02/12 06:55:58 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 9  total_loss: 7.325  loss_cls: 2.508  loss_box_reg: 0.1683  loss_mask: 0.6894  loss_rpn_cls: 3.279  loss_rpn_loc: 0.588    time: 0.3318  last_time: 0.3730  data_time: 0.4346  last_data_time: 0.0014   lr: 0.00022503  max_mem: 4671M\n",
      "\u001b[32m[02/12 06:55:58 d2.engine.hooks]: \u001b[0mOverall training speed: 8 iterations in 0:00:02 (0.3318 s / it)\n",
      "\u001b[32m[02/12 06:55:58 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:03 (0:00:00 on hooks)\n"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "# Mask R-CNN 학습 설정\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\n",
    "    model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    ")\n",
    "cfg.DATASETS.TRAIN = (\"custom_doclaynet\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
    "    \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    ")  # 사전학습 모델 사용\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.SOLVER.MAX_ITER = 10  # 학습 반복 횟수\n",
    "cfg.MODEL.MASK_ON = True\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(dataset_metadata.thing_classes)\n",
    "\n",
    "# 학습 실행\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델이 저장되었습니다: C:\\Users\\nature\\Documents\\detectron2\\trained_model\\mask_rcnn_model.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 모델 저장 경로 설정\n",
    "output_dir = cfg.OUTPUT_DIR\n",
    "model_final_path = os.path.join(output_dir, \"model_final.pth\")  # 최종 모델 파일\n",
    "\n",
    "# 모델이 존재하는지 확인 후 백업\n",
    "if os.path.exists(model_final_path):\n",
    "    save_path = r\"C:\\Users\\nature\\Documents\\detectron2\\trained_model\\mask_rcnn_model.pth\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    shutil.copy(model_final_path, save_path)\n",
    "    print(f\"✅ 모델이 저장되었습니다: {save_path}\")\n",
    "else:\n",
    "    print(\"⚠️ 모델 파일을 찾을 수 없습니다. 학습이 완료되었는지 확인하세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU 사용 가능: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ GPU가 비활성화됨! CPU에서 실행 중.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Detectron2 설정에서 GPU 사용하도록 강제 설정\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mcfg\u001b[49m\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mDEVICE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cfg' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU 사용 가능 여부 확인\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ GPU 사용 가능:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"⚠️ GPU가 비활성화됨! CPU에서 실행 중.\")\n",
    "\n",
    "# Detectron2 설정에서 GPU 사용하도록 강제 설정\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/12 07:03:18 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from C:\\Users\\nature\\Documents\\detectron2\\trained_model\\mask_rcnn_model.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (13, 1024) in the checkpoint but (81, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (13,) in the checkpoint but (81,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (48, 1024) in the checkpoint but (320, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (48,) in the checkpoint but (320,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (12, 256, 1, 1) in the checkpoint but (80, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (12,) in the checkpoint but (80,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n",
      "c:\\Users\\nature\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\torch\\functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3638.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import detectron2\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "save_path =r\"C:\\Users\\nature\\Documents\\detectron2\\trained_model\\mask_rcnn_model.pth\"\n",
    "# 모델 로드\n",
    "# Mask R-CNN 학습 설정\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\n",
    "    model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    ")\n",
    "cfg.MODEL.WEIGHTS = save_path  # 학습된 모델 가중치 불러오기\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # 신뢰도 임계값 설정\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # GPU 사용 가능 여부 확인\n",
    "\n",
    "# 예측을 위한 Predictor 생성\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# 테스트할 이미지 불러오기\n",
    "test_image_path = r\"C:\\Users\\nature\\Documents\\detectron2\\dp-bench\\dataset\\images\\01030000000037_page_1.jpg\"\n",
    "image = cv2.imread(test_image_path)\n",
    "\n",
    "# 예측 실행\n",
    "outputs = predictor(image)\n",
    "\n",
    "# 결과 시각화\n",
    "v = Visualizer(image[:, :, ::-1], metadata=MetadataCatalog.get(\"custom_doclaynet\"), scale=1.0)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "# 예측된 이미지 출력\n",
    "cv2.imshow(\"Predictions\", v.get_image()[:, :, ::-1])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 3. 학습된 모델을 나중에 다시 로드하는 방법\n",
    "이제 저장된 모델을 나중에 다시 로드하여 추가 훈련하거나 다른 데이터에 대해 예측할 수도 있어."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DefaultPredictor\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 기존 모델 불러오기\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mcfg\u001b[49m\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mWEIGHTS \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mnature\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdetectron2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrained_model\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmask_rcnn_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m predictor \u001b[38;5;241m=\u001b[39m DefaultPredictor(cfg)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 새로운 이미지 예측\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cfg' is not defined"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "# 기존 모델 불러오기\n",
    "cfg.MODEL.WEIGHTS = r\"C:\\Users\\nature\\Documents\\detectron2\\trained_model\\mask_rcnn_model.pth\"\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# 새로운 이미지 예측\n",
    "new_image_path = r\"C:\\Users\\nature\\Documents\\detectron2\\doclaynet\\images\\01030000000045_page_2.jpg\"\n",
    "image = cv2.imread(new_image_path)\n",
    "outputs = predictor(image)\n",
    "\n",
    "# 결과 확인\n",
    "v = Visualizer(image[:, :, ::-1], metadata=MetadataCatalog.get(\"custom_doclaynet\"), scale=1.0)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "cv2.imshow(\"Predictions\", v.get_image()[:, :, ::-1])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
